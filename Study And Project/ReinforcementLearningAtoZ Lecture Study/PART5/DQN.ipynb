{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "from src.part3.MLP import MultiLayerPerceptron as MLP\n",
    "from src.part5.DQN import DQN, prepare_training_inputs\n",
    "from src.common.memory.memory import ReplayMemory\n",
    "from src.common.train_utils import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4 * 5\n",
    "batch_size = 256\n",
    "gamma = 1.0\n",
    "memory_size = 50000\n",
    "total_eps = 3000\n",
    "eps_max = 0.08\n",
    "eps_min = 0.01\n",
    "sampling_only_until = 2000\n",
    "target_update_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joon0\\Anaconda3\\envs\\gpu_torch130\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "qnet = MLP(4, 2, num_neurons=[128])\n",
    "qnet_target = MLP(4, 2, num_neurons=[128])\n",
    "\n",
    "# initialize target network same as the main network.\n",
    "qnet_target.load_state_dict(qnet.state_dict())\n",
    "agent = DQN(4, 1, qnet=qnet, qnet_target=qnet_target, lr=lr, gamma=gamma, epsilon=1.0)\n",
    "env = gym.make('CartPole-v1')\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    0 | Cumulative Reward :   16 | Epsilon : 0.080\n",
      "Episode :  100 | Cumulative Reward :   12 | Epsilon : 0.075\n",
      "Episode :  200 | Cumulative Reward :    9 | Epsilon : 0.070\n",
      "Episode :  300 | Cumulative Reward :   49 | Epsilon : 0.065\n",
      "Episode :  400 | Cumulative Reward :   66 | Epsilon : 0.060\n",
      "Episode :  500 | Cumulative Reward :   55 | Epsilon : 0.055\n",
      "Episode :  600 | Cumulative Reward :  275 | Epsilon : 0.050\n",
      "Episode :  700 | Cumulative Reward :  160 | Epsilon : 0.045\n",
      "Episode :  800 | Cumulative Reward :   53 | Epsilon : 0.040\n",
      "Episode :  900 | Cumulative Reward :   67 | Epsilon : 0.035\n",
      "Episode : 1000 | Cumulative Reward :  147 | Epsilon : 0.030\n",
      "Episode : 1100 | Cumulative Reward :   61 | Epsilon : 0.025\n",
      "Episode : 1200 | Cumulative Reward :   54 | Epsilon : 0.020\n",
      "Episode : 1300 | Cumulative Reward :  221 | Epsilon : 0.015\n",
      "Episode : 1400 | Cumulative Reward :   85 | Epsilon : 0.010\n",
      "Episode : 1500 | Cumulative Reward :   31 | Epsilon : 0.010\n",
      "Episode : 1600 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1700 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1800 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1900 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2000 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2100 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2200 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2300 | Cumulative Reward :  222 | Epsilon : 0.010\n",
      "Episode : 2400 | Cumulative Reward :  326 | Epsilon : 0.010\n",
      "Episode : 2500 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2600 | Cumulative Reward :  486 | Epsilon : 0.010\n",
      "Episode : 2700 | Cumulative Reward :  277 | Epsilon : 0.010\n",
      "Episode : 2800 | Cumulative Reward :   11 | Epsilon : 0.010\n",
      "Episode : 2900 | Cumulative Reward :   13 | Epsilon : 0.010\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "\n",
    "for n_epi in range(total_eps):\n",
    "    # epsilon scheduling\n",
    "    # slowly decaying_epsilon\n",
    "    epsilon = max(eps_min, eps_max - eps_min * (n_epi / 200))\n",
    "    agent.epsilon = torch.tensor(epsilon)\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(1, 4))\n",
    "        a = agent.get_action(s)\n",
    "        ns, r, done, info = env.step(a)\n",
    "\n",
    "        experience = (s,\n",
    "                      torch.tensor(a).view(1, 1),\n",
    "                      torch.tensor(r / 100.0).view(1, 1),\n",
    "                      torch.tensor(ns).view(1, 4),\n",
    "                      torch.tensor(done).view(1, 1))\n",
    "        memory.push(experience)\n",
    "\n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if len(memory) >= sampling_only_until:\n",
    "        # train agent\n",
    "        sampled_exps = memory.sample(batch_size)\n",
    "        sampled_exps = prepare_training_inputs(sampled_exps)\n",
    "        agent.update(*sampled_exps)\n",
    "\n",
    "    if n_epi % target_update_interval == 0:\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "    \n",
    "    if n_epi % print_every == 0:\n",
    "        msg = (n_epi, cum_r, epsilon)\n",
    "        print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
